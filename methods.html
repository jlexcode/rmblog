<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Methodology & Validation - Reasonable Machines</title>
    <meta name="description" content="Methods behind the predictions: data sources, features, modeling choices, training, evaluation, and limitations.">
    <meta name="keywords" content="Supreme Court predictions, methodology, validation, model evaluation, legal AI">
    <meta name="author" content="Jed Stiglitz">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://reasonablemachines.io/methods.html">
    <meta property="og:title" content="Methodology & Validation - Reasonable Machines">
    <meta property="og:description" content="Methods behind the predictions: data, modeling, evaluation, and limitations.">
    <meta property="og:site_name" content="Reasonable Machines">
    
    <!-- Twitter -->
    <meta property="twitter:card" content="summary">
    <meta property="twitter:url" content="https://reasonablemachines.io/methods.html">
    <meta property="twitter:title" content="Methodology & Validation - Reasonable Machines">
    <meta property="twitter:description" content="Methods behind the predictions: data, modeling, evaluation, and limitations.">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="https://reasonablemachines.io/methods.html">
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/@supabase/supabase-js@2"></script>
    <link href="https://fonts.googleapis.com/css2?family=Space+Mono:wght@400;700&family=Inter:wght@400;500&display=swap" rel="stylesheet">
    <link href="styles.css" rel="stylesheet">
</head>
<body class="bg-white text-black font-sans">
    <div class="max-w-2xl mx-auto px-4 py-8">
        <div id="header"></div>
        
        <header class="mb-8">
            <h1 class="text-3xl font-normal text-black mb-3 font-['Space_Mono']">Methodology & Validation</h1>
        </header>

        <main class="space-y-6">
            <article class="prose prose-lg max-w-none" id="basic-approach">

                <h2 class="text-xl font-normal text-black mb-3 font-['Space_Mono']">Introduction</h2>
                <div class="text-black leading-relaxed font-['Inter']">
                    <p class="mb-4">
                    A primary focus of this research log is to experiment with legal forecasting using recent transformer-based models. This page documents the methodology and validation of the model. This page may update with improvements in the model. (I use "model" permissively to include the pipeline from raw data to inference and ensembling.) 


                <h2 class="text-xl font-normal text-black mb-3 font-['Space_Mono']">Basic Approach</h2>
                <div class="text-black leading-relaxed font-['Inter']">
                    <p class="mb-4">The objective of the model development is to learn the justices' personalities and preferences. To do so, I experiment with signals from party filings, lower court decisions, oral argument, and amicus briefs, and train adapters of large-parameter open-weight models. For the validation exercise reported here, I trained on data from terms since Justice Kagan took her seat but stopped in OT2023. This allows us to evaluate model performance against OT2024, the term that wrapped up in June 2025. Information can leak across cases within terms, and it is important to validate against data that is temporally cordoned from the training data rather than use standard cross validation.</p>
                </div>

                <h2 class="text-xl font-normal text-black mb-3 font-['Space_Mono']">Benchmark Performance</h2>

                <div class="text-black leading-relaxed font-['Inter']">
                    <p class="mb-4">It is notoriously difficult to predict Supreme Court votes. The leading <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0174698&trk=public_post_comment-text" target="_blank" rel="noopener noreferrer" class="post-link">algorithmic approaches</a> produce vote-level predictions with about 70 percent accuracy. Wisdom of <a href="https://fantasyscotus.net/case/list/" target="_blank" rel="noopener noreferrer" class="post-link">human crowds</a> tends to be higher—about 80 percent accuracy. But Case-level predictions tend to be lower. For example, last term (OT2024) the human crowds predicted vote outcomes with an accuracy of 80.6 percent, but case-level outcomes with an accuracy of only 76.3 percent.</p>
                    <p class="mb-4">Both the algorithmic and human results include near-decision information, such as oral argument data, amicus briefs, etc. The main model I develop uses most near-decision information, too. However, I only use features available up to the day of oral argument. This means that I exclude information that others include, perhaps most notably the timing of the decision itself. For my prediction objectives, I view that information as a kind of leakage—the timing of the decision may be endogenous to characteristics we are trying to model.</p>
                </div>

                <h2 class="text-xl font-normal text-black mb-3 font-['Space_Mono']">Validation</h2>

                <div class="text-black leading-relaxed font-['Inter']">
                    <p class="mb-4">Model performance is validated against a hold-out term, OT2024. This is the term that wrapped up in June 2025. Performance on this test term will provide a useful assessment of how the model will perform on the current term. (Note that for the current term predictions, I roll training forward to include data from last term.)</p>
                    <p class="mb-4">The results for OT2024 are reported in the table below. The model’s vote-level accuracy is 71 percent, and its case-level accuracy is 79 percent. Vote-level accuracy therefore is about the same as algorithimic SOTA, though it lags behind the human crowds. Case-level accuracy is higher for the model, reflecting the model’s ability to capture the nuances of the cases. The model beats both algorithmic SOTA and human crowds at the case level.</p>
                    <p class="mb-4">It is common to assess performance using the F1 score, along with recall and precision. Recall is the proportion of positive instances correctly identified; and precision tells us the proportion of cases the model marks as true are in fact true. Together, they give a measure of false positives and negatives, and the F1 score is the (harmonic) average of the two.</p>
                    <p class="mb-4">Overall, at the vote level, F1 score is 0.71 when weighted by class frequency and 0.67 when unweighted by frequency. The affirm class is more difficult to predict and that shows up in the table through a lower F1 score.</p>
<br><br><br><div class="overflow-x-auto my-2"><table class="w-full border-collapse border border-gray-300"><thead><tr class="bg-gray-100"><th class="border border-gray-300 px-3 py-2 text-left font-['Space_Mono'] font-medium">Class</th><th class="border border-gray-300 px-3 py-2 text-left font-['Space_Mono'] font-medium">Precision</th><th class="border border-gray-300 px-3 py-2 text-left font-['Space_Mono'] font-medium">Recall</th><th class="border border-gray-300 px-3 py-2 text-left font-['Space_Mono'] font-medium">F1</th><th class="border border-gray-300 px-3 py-2 text-left font-['Space_Mono'] font-medium">Support</th></tr></thead><tbody><tr class="hover:bg-gray-50"><td class="border border-gray-300 px-3 py-2 font-['Inter']">Affirm</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.6</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.49</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.54</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">173</td></tr><tr class="hover:bg-gray-50"><td class="border border-gray-300 px-3 py-2 font-['Inter']">Reverse</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.76</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.83</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.79</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">328</td></tr><tr class="hover:bg-gray-50"><td class="border border-gray-300 px-3 py-2 font-['Inter']">Macro (weighted)</td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.71</td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td></tr><tr class="hover:bg-gray-50"><td class="border border-gray-300 px-3 py-2 font-['Inter']">Macro (unweighted)</td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.67</td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td></tr></tbody></table><p class="text-sm text-gray-600 mt-1 text-center font-['Inter'] italic">Table 1: Vote-level performance</p></div><br><br>

                    <p class="mb-4">Also important is model calibration. Calibration refers to the model's ability to tell us how confident it is in its predictions. So if it says there is a fifty percent chance of an event, does the observed event happen fifty percent of the time?</p>
                    <p class="mb-4">Figure 1 bins the predicted probabilities from the model for reversal and shows how often, within each bin, the vote turns out to be reversal. A perfectly calibrated model would have points for each bin exactly on the 45-degree angle line. The observed calibration is roughly along that line.</p>
                    <figure class="my-2"><img src="https://xlglobsjkfpfpkxlivki.supabase.co/storage/v1/object/public/blog-images/1757617799175-76lgy0.png" alt="Vote-level calibration" class="w-full rounded-lg shadow-sm"><figcaption class="text-sm text-gray-600 mt-1 text-center font-['Inter']">Figure 1: Vote-level calibration</figcaption></figure>
                    <p class="mb-4">The expected calibration error (ECE), or departure from the line of perfect correspondence, is 0.04—the average bin is about four percentage points off target. We can also look at the Brier score, which is the mean square of the difference of the label and the predicted probability. Lower is better. The model's Brier score is 0.19. That can be used later to compare to other models.</p>

                    Unlike other models, including human models, this model performs better at the case level than the vote-level. Consider table 2, which reports model performance by class at the case level. All relevant measures are stronger at the case level than the vote level. On calibration, the ECE is somewhat worse (0.10) but the Brier score is somewhat stronger (0.17). <br><br><br><div class="overflow-x-auto my-2"><table class="w-full border-collapse border border-gray-300"><thead><tr class="bg-gray-100"><th class="border border-gray-300 px-3 py-2 text-left font-['Space_Mono'] font-medium">Class</th><th class="border border-gray-300 px-3 py-2 text-left font-['Space_Mono'] font-medium">Precision</th><th class="border border-gray-300 px-3 py-2 text-left font-['Space_Mono'] font-medium">Recall</th><th class="border border-gray-300 px-3 py-2 text-left font-['Space_Mono'] font-medium">F1</th><th class="border border-gray-300 px-3 py-2 text-left font-['Space_Mono'] font-medium">Support</th></tr></thead><tbody><tr class="hover:bg-gray-50"><td class="border border-gray-300 px-3 py-2 font-['Inter']">Affirm</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.64</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.47</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.54</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">15</td></tr><tr class="hover:bg-gray-50"><td class="border border-gray-300 px-3 py-2 font-['Inter']">Reverse</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.82</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.9</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.86</td><td class="border border-gray-300 px-3 py-2 font-['Inter']">41</td></tr><tr class="hover:bg-gray-50"><td class="border border-gray-300 px-3 py-2 font-['Inter']">Macro (weighted)</td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.77</td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td></tr><tr class="hover:bg-gray-50"><td class="border border-gray-300 px-3 py-2 font-['Inter']">Macro (unweighted)</td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td><td class="border border-gray-300 px-3 py-2 font-['Inter']">0.7</td><td class="border border-gray-300 px-3 py-2 font-['Inter']"></td></tr></tbody></table><p class="text-sm text-gray-600 mt-1 text-center font-['Inter'] italic">Table 2: Case-level performance</p></div><br><br>
                </div>

                <h2 class="text-xl font-normal text-black mb-3 font-['Space_Mono']">Limitations</h2>
                <div class="text-black leading-relaxed font-['Inter']">
                
                    <p class="mb-4">The model performs well relative to benchmarks on OT2024. However, this is a difficult task and there are limits in model performance. Here are some of the main limitations:
                        <ul class="list-disc list-inside">
                            <li>Drift. It is possible that the feature's that are predictive of the Court's behavior will change over time. This could be because the docket changes, the composition of the Court changes, or the decision-environment changes.</li>
                            <br>
                            <li>Generalizability. Even without drift, it is possible that OT2024 is unrepresentative of the Court's behavior in the current term. The Court's behavior may be more similar to OT2023 or OT2019.</li>
                            <br>
                            <li>Explainability. The model contains tens of billions of parameters and is therefore difficult to interpret. Part of this research exercise is to investigate the model's behavior and engage close readings of the underlying material.</li>
                        </ul>
                    </p>



            </article>
        </main>
    </div>

    <script src="header.js"></script>

    <!-- PostHog Analytics -->
    <script>
        !function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]);t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.async=!0,p.src=s.api_host+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="capture identify alias people.set people.set_once set_config register register_once unregister opt_out_capturing has_opted_out_capturing opt_in_capturing reset isFeatureEnabled onFeatureFlags getFeatureFlag getFeatureFlagPayload reloadFeatureFlags group updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures getActiveMatchingSurveys getSurveys".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
        posthog.init('phc_wfVnnyCEXjwV0azeFP8TlojFUL83RJ3j9WWlSxMV9VQ', {api_host: 'https://app.posthog.com'})
        
        // Track page view
        posthog.capture('page_viewed', {
            page_title: 'Methodology & Validation',
            page_type: 'methods',
            page_url: window.location.href
        })
    </script>
</body>
</html>


